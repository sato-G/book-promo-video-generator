#!/usr/bin/env python3
"""
æ›¸ç±åˆ†æãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆç”»é¢1ç”¨ï¼‰

EPUB â†’ ãƒ†ã‚­ã‚¹ãƒˆåŒ– â†’ ãƒãƒ£ãƒ³ã‚¯åŒ– â†’ ãƒãƒ£ãƒ³ã‚¯ã¾ã¨ã‚ â†’ å…¨ä½“æ¦‚è¦ï¼ˆè«–æ–‡å½¢å¼800å­—ï¼‰
"""

from pathlib import Path
from typing import Dict, Any, List
import google.generativeai as genai
import os
import json
import time
from dotenv import load_dotenv
import ebooklib
from ebooklib import epub
from bs4 import BeautifulSoup

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()


def extract_text_from_epub(epub_path: Path) -> str:
    """EPUBã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    book = epub.read_epub(str(epub_path))
    text_content = []

    for item in book.get_items():
        if item.get_type() == ebooklib.ITEM_DOCUMENT:
            soup = BeautifulSoup(item.get_content(), 'html.parser')
            text = soup.get_text(separator='\n', strip=True)
            if text:
                text_content.append(text)

    return '\n\n'.join(text_content)


def chunk_text(text: str, chunk_size: int = 40000) -> List[str]:
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ï¼ˆ40000æ–‡å­—ãšã¤ï¼‰"""
    chunks = []
    current_chunk = ""
    paragraphs = text.split('\n\n')

    for para in paragraphs:
        if len(current_chunk) + len(para) <= chunk_size:
            current_chunk += para + "\n\n"
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = para + "\n\n"

    if current_chunk:
        chunks.append(current_chunk.strip())

    return chunks


def summarize_chunks(chunks: List[str]) -> List[str]:
    """å„ãƒãƒ£ãƒ³ã‚¯ã‚’1000-1500æ–‡å­—ã«ã¾ã¨ã‚ã‚‹"""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-flash-lite')

    summaries = []

    for i, chunk in enumerate(chunks):
        print(f"  ğŸ“ ãƒãƒ£ãƒ³ã‚¯{i+1}/{len(chunks)}ã‚’ã¾ã¨ã‚ä¸­...")

        prompt = f"""
ä»¥ä¸‹ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’1000-1500æ–‡å­—ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

{chunk}

**è¦ä»¶:**
- å®¢è¦³çš„ãƒ»ä¸­ç«‹çš„ã«
- ä¸»è¦ãªå†…å®¹ã‚’æ¼ã‚‰ã•ãšå«ã‚ã‚‹
- äº‹å®Ÿãƒ™ãƒ¼ã‚¹
- è©³ç´°ãªè¦ç´„

1000-1500æ–‡å­—ã®è¦ç´„ã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
"""

        response = model.generate_content(
            prompt,
            generation_config={"temperature": 0.3}
        )

        summaries.append(response.text.strip())

        # APIåˆ¶é™å›é¿ã®ãŸã‚ã€ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é…å»¶ã‚’è¿½åŠ 
        if i < len(chunks) - 1:  # æœ€å¾Œã®ãƒãƒ£ãƒ³ã‚¯ä»¥å¤–
            time.sleep(2)  # 2ç§’å¾…æ©Ÿ

    return summaries


def generate_final_summary(chunk_summaries: List[str], book_name: str) -> Dict[str, Any]:
    """ãƒãƒ£ãƒ³ã‚¯ã¾ã¨ã‚ã‹ã‚‰å…¨ä½“æ¦‚è¦ã‚’ç”Ÿæˆï¼ˆè«–æ–‡å½¢å¼800å­—ï¼‰"""
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("GOOGLE_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-2.5-flash-lite')

    all_summaries = '\n\n'.join([f"ã€éƒ¨åˆ†{i+1}ã€‘\n{s}" for i, s in enumerate(chunk_summaries)])

    prompt = f"""
ã‚ãªãŸã¯å­¦è¡“è«–æ–‡ã®è¦æ—¨ã‚’æ›¸ãå°‚é–€å®¶ã§ã™ã€‚
ä»¥ä¸‹ã¯æ›¸ç±ã€Œ{book_name}ã€ã®å„éƒ¨åˆ†ã®è¦ç´„ã§ã™ã€‚ã“ã‚Œã‚’èª­ã¿ã€**è«–æ–‡ã®è¦æ—¨ï¼ˆã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆï¼‰å½¢å¼**ã§å…¨ä½“æ¦‚è¦ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚

## å„éƒ¨åˆ†ã®è¦ç´„
{all_summaries}

---

## ã‚¿ã‚¹ã‚¯

ã“ã®æ›¸ç±ã«ã¤ã„ã¦ã€**800æ–‡å­—ç¨‹åº¦**ã®å®¢è¦³çš„ãªæ¦‚è¦ã‚’è«–æ–‡å½¢å¼ã§ä½œæˆã—ã¦ãã ã•ã„ã€‚

### è¦ä»¶

1. **è«–æ–‡ã®è¦æ—¨å½¢å¼**
   - å®¢è¦³çš„ãƒ»ä¸­ç«‹çš„ãªè¨˜è¿°
   - ã€Œã§ã‚ã‚‹ã€èª¿
   - å®£ä¼çš„ãªè¡¨ç¾ã¯ä¸€åˆ‡ä½¿ã‚ãªã„

2. **å«ã‚ã‚‹ã¹ãå†…å®¹**
   - æ›¸ç±ã®ä¸»é¡Œãƒ»ãƒ†ãƒ¼ãƒ
   - æ‰±ã£ã¦ã„ã‚‹å†…å®¹ã®æ¦‚è¦
   - ä¸»è¦ãªè«–ç‚¹
   - æ›¸ç±ã®æ§‹æˆ
   - å¯¾è±¡èª­è€…å±¤ï¼ˆå®¢è¦³çš„ã«ï¼‰

3. **é¿ã‘ã‚‹ã¹ãè¡¨ç¾**
   - ã€Œé¢ç™½ã„ã€ã€Œæ„Ÿå‹•çš„ã€ãªã©ã®ä¸»è¦³çš„è©•ä¾¡
   - ã€Œå¿…èª­ã€ã€ŒãŠã™ã™ã‚ã€ãªã©ã®å®£ä¼æ–‡å¥
   - èª­è€…ã¸ã®å‘¼ã³ã‹ã‘

4. **æ–‡å­—æ•°: 700-900æ–‡å­—**

---

JSONå½¢å¼ã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›å½¢å¼:
{{
  "summary": "è«–æ–‡å½¢å¼ã®æ¦‚è¦æœ¬æ–‡",
  "character_count": å®Ÿéš›ã®æ–‡å­—æ•°,
  "main_topics": ["ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯1", "ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯2", "ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯3"],
  "target_audience": "æƒ³å®šã•ã‚Œã‚‹èª­è€…å±¤",
  "book_type": "æ›¸ç±ã®ç¨®é¡"
}}
"""

    print(f"  ğŸ¤– å…¨ä½“æ¦‚è¦ã‚’ç”Ÿæˆä¸­...")
    response = model.generate_content(
        prompt,
        generation_config={
            "temperature": 0.3,
            "response_mime_type": "application/json"
        }
    )

    result = json.loads(response.text)
    print(f"  âœ“ å…¨ä½“æ¦‚è¦ç”Ÿæˆå®Œäº†ï¼ˆ{result['character_count']}æ–‡å­—ï¼‰")

    return result


def analyze_book(epub_path: Path, output_dir: Path) -> Dict[str, Any]:
    """
    æ›¸ç±ã‚’åˆ†æï¼ˆç”»é¢1ã®å…¨å‡¦ç†ï¼‰

    1. EPUBã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    2. ãƒãƒ£ãƒ³ã‚¯åŒ–
    3. ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ã¾ã¨ã‚
    4. å…¨ä½“æ¦‚è¦ç”Ÿæˆï¼ˆè«–æ–‡å½¢å¼800å­—ï¼‰

    Returns:
        åˆ†æçµæœã®è¾æ›¸
    """
    print(f"\n{'='*80}")
    print(f"ğŸ“š æ›¸ç±åˆ†æé–‹å§‹: {epub_path.name}")
    print(f"{'='*80}\n")

    # 1. ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    print("ğŸ“– Step 1/4: ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºä¸­...")
    full_text = extract_text_from_epub(epub_path)
    print(f"  âœ“ {len(full_text)}æ–‡å­—ã‚’æŠ½å‡º")

    book_name = epub_path.stem

    # ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜
    output_dir.mkdir(parents=True, exist_ok=True)
    text_file = output_dir / f"{book_name}.txt"
    with open(text_file, 'w', encoding='utf-8') as f:
        f.write(full_text)

    # 2. ãƒãƒ£ãƒ³ã‚¯åŒ–
    print("\nğŸ” Step 2/4: ãƒãƒ£ãƒ³ã‚¯åŒ–ä¸­...")
    chunks = chunk_text(full_text, chunk_size=2000)
    print(f"  âœ“ {len(chunks)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²")

    # 3. ãƒãƒ£ãƒ³ã‚¯ã¾ã¨ã‚
    print("\nğŸ“ Step 3/4: å„ãƒãƒ£ãƒ³ã‚¯ã‚’ã¾ã¨ã‚ä¸­...")
    chunk_summaries = summarize_chunks(chunks)
    print(f"  âœ“ {len(chunk_summaries)}å€‹ã®ã¾ã¨ã‚ã‚’ç”Ÿæˆ")

    # 4. å…¨ä½“æ¦‚è¦ç”Ÿæˆ
    print("\nâœ¨ Step 4/4: å…¨ä½“æ¦‚è¦ã‚’ç”Ÿæˆä¸­...")
    final_summary = generate_final_summary(chunk_summaries, book_name)

    # çµæœã‚’ã¾ã¨ã‚ã‚‹
    result = {
        "book_name": book_name,
        "text_file": str(text_file),
        "character_count": len(full_text),
        "num_chunks": len(chunks),
        "chunk_summaries": chunk_summaries,
        **final_summary
    }

    # data/internal/ã«ä¿å­˜
    from .utils import get_project_root, save_json

    internal_dir = get_project_root() / "data" / "internal"
    internal_dir.mkdir(parents=True, exist_ok=True)

    analysis_file = internal_dir / "book_analysis.json"
    save_json(analysis_file, result)

    print(f"\n{'='*80}")
    print(f"âœ… åˆ†æå®Œäº†ï¼")
    print(f"{'='*80}\n")

    return result
