#!/usr/bin/env python3
"""
å‹•ç”»ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ« v2

moviepyã‚’ä½¿ç”¨ã—ã¦ã€ã‚·ãƒ¼ãƒ³ç”»åƒã¨éŸ³å£°ã‹ã‚‰ç›´æ¥å‹•ç”»ã‚’ç”Ÿæˆ
v1ã‚¹ã‚¯ãƒªãƒ—ãƒˆã«ã¯ä¾å­˜ã—ãªã„
"""

from pathlib import Path
from typing import Dict, Any, List
import json
import subprocess

# moviepy 2.x
from moviepy import (
    ImageClip, AudioFileClip, CompositeVideoClip,
    concatenate_videoclips, TextClip, CompositeAudioClip,
    VideoFileClip
)
from moviepy import vfx

from . import subtitle_generator
import random


def get_project_root() -> Path:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’å–å¾—"""
    return Path(__file__).parent.parent


def apply_ken_burns_effect(clip, effect_type: str = "zoom_in", intensity: float = 1.15):
    """
    Ken Burnsã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‚’é©ç”¨ï¼ˆã‚ºãƒ¼ãƒ ï¼†ãƒ‘ãƒ³ï¼‰

    Args:
        clip: ImageClip
        effect_type: "zoom_in", "zoom_out", "pan_left", "pan_right", "random"
        intensity: æ‹¡å¤§ç‡ï¼ˆ1.0ï½1.3æ¨å¥¨ï¼‰

    Returns:
        ã‚¨ãƒ•ã‚§ã‚¯ãƒˆé©ç”¨å¾Œã®ã‚¯ãƒªãƒƒãƒ—
    """
    if effect_type == "random":
        effect_type = random.choice(["zoom_in", "zoom_out", "pan_left", "pan_right"])

    w, h = clip.size
    duration = clip.duration

    if effect_type == "zoom_in":
        # ã‚ºãƒ¼ãƒ ã‚¤ãƒ³: å°â†’å¤§
        # resizeã‚’æ™‚é–“é–¢æ•°ã¨ã—ã¦ä½¿ç”¨
        zoomed_clip = clip.resized(lambda t: 1.0 + (intensity - 1.0) * (t / duration))
        return zoomed_clip

    elif effect_type == "zoom_out":
        # ã‚ºãƒ¼ãƒ ã‚¢ã‚¦ãƒˆ: å¤§â†’å°
        zoomed_clip = clip.resized(lambda t: intensity - (intensity - 1.0) * (t / duration))
        return zoomed_clip

    elif effect_type == "pan_left":
        # å·¦â†’å³ãƒ‘ãƒ³ï¼ˆç”»åƒã‚’æ‹¡å¤§ã—ã¦å·¦ã‹ã‚‰å³ã«ç§»å‹•ï¼‰
        # ç”»åƒã‚’æ‹¡å¤§ã—ã¦ã‹ã‚‰ä½ç½®ã‚’å‹•ã‹ã™
        scaled_clip = clip.resized(intensity)
        # å·¦ç«¯ã‹ã‚‰å³ç«¯ã¸ç§»å‹•
        pan_offset = int(w * (intensity - 1))
        panned_clip = scaled_clip.with_position(lambda t: (-pan_offset + int(pan_offset * t / duration), "center"))
        return CompositeVideoClip([panned_clip], size=(w, h))

    elif effect_type == "pan_right":
        # å³â†’å·¦ãƒ‘ãƒ³ï¼ˆç”»åƒã‚’æ‹¡å¤§ã—ã¦å³ã‹ã‚‰å·¦ã«ç§»å‹•ï¼‰
        scaled_clip = clip.resized(intensity)
        pan_offset = int(w * (intensity - 1))
        panned_clip = scaled_clip.with_position(lambda t: (-int(pan_offset * t / duration), "center"))
        return CompositeVideoClip([panned_clip], size=(w, h))

    return clip


def render_video(
    storyboard_data: Dict[str, Any],
    subtitle_type: str = "normal",
    subtitle_colors: tuple = ("FFFFFF", "00FFFF"),
    use_ken_burns: bool = False,
    ken_burns_type: str = "random",
    ken_burns_intensity: float = 1.15,
    transition_type: str = "ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰",
    transition_duration: float = 0.8
) -> Dict[str, Any]:
    """
    ã‚¹ãƒˆãƒ¼ãƒªãƒ¼ãƒœãƒ¼ãƒ‰ã‹ã‚‰å‹•ç”»ã‚’ä½œæˆï¼ˆå­—å¹•ä»˜ãï¼‰

    Args:
        storyboard_data: ã‚·ãƒ¼ãƒ³æƒ…å ±
            {
                'book_name': str,
                'total_scenes': int,
                'scenes': [
                    {
                        'scene_number': int,
                        'narration': str,
                        'image_file': str,
                        'audio_file': str,
                        'duration': float
                    }
                ]
            }
        subtitle_type: å­—å¹•ã‚¿ã‚¤ãƒ— ("karaoke" or "normal")

    Returns:
        ç”Ÿæˆã•ã‚ŒãŸå‹•ç”»æƒ…å ±
    """
    project_root = get_project_root()
    book_name = storyboard_data['book_name']
    scenes = storyboard_data['scenes']

    print(f"ğŸ¬ å‹•ç”»ç”Ÿæˆé–‹å§‹: {book_name}")
    print(f"   ã‚·ãƒ¼ãƒ³æ•°: {len(scenes)}")

    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
    output_dir = project_root / "data" / "output" / "videos" / book_name
    output_dir.mkdir(parents=True, exist_ok=True)

    # å„ã‚·ãƒ¼ãƒ³ã®ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
    clips = []

    for scene in scenes:
        scene_num = scene['scene_number']
        image_file = Path(scene['image_file'])
        audio_file = Path(scene['audio_file'])

        if not image_file.exists():
            raise FileNotFoundError(f"ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {image_file}")
        if not audio_file.exists():
            raise FileNotFoundError(f"éŸ³å£°ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {audio_file}")

        print(f"   ã‚·ãƒ¼ãƒ³{scene_num}ã‚’å‡¦ç†ä¸­...")

        # éŸ³å£°ã‹ã‚‰å®Ÿéš›ã®é•·ã•ã‚’å–å¾—
        audio_clip = AudioFileClip(str(audio_file))
        duration = audio_clip.duration

        # ç”»åƒã‚¯ãƒªãƒƒãƒ—ä½œæˆï¼ˆéŸ³å£°ã®é•·ã•ã«åˆã‚ã›ã‚‹ï¼‰
        image_clip = ImageClip(str(image_file)).with_duration(duration)

        # Ken Burnsã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‚’é©ç”¨
        if use_ken_burns:
            # ã‚¨ãƒ•ã‚§ã‚¯ãƒˆã‚¿ã‚¤ãƒ—ã‚’æ—¥æœ¬èªã‹ã‚‰è‹±èªã«å¤‰æ›
            effect_mapping = {
                "ã‚ºãƒ¼ãƒ ã‚¤ãƒ³": "zoom_in",
                "ã‚ºãƒ¼ãƒ ã‚¢ã‚¦ãƒˆ": "zoom_out",
                "å·¦â†’å³ãƒ‘ãƒ³": "pan_left",
                "å³â†’å·¦ãƒ‘ãƒ³": "pan_right",
                "ãƒ©ãƒ³ãƒ€ãƒ ": "random"
            }
            effect_type_en = effect_mapping.get(ken_burns_type, "random")

            print(f"     Ken Burnsã‚¨ãƒ•ã‚§ã‚¯ãƒˆé©ç”¨: {ken_burns_type} (å¼·åº¦: {ken_burns_intensity})")
            image_clip = apply_ken_burns_effect(image_clip, effect_type_en, ken_burns_intensity)

        # éŸ³å£°ã‚’è¨­å®š
        image_clip = image_clip.with_audio(audio_clip)

        # ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ãƒ»ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆï¼ˆæœ€åˆã¨æœ€å¾Œã®ã‚·ãƒ¼ãƒ³ã®ã¿ï¼‰
        if scene_num == 1:
            image_clip = image_clip.with_effects([vfx.FadeIn(0.5)])
        if scene_num == len(scenes):
            image_clip = image_clip.with_effects([vfx.FadeOut(0.5)])

        clips.append(image_clip)

    # å…¨ã‚·ãƒ¼ãƒ³ã‚’é€£çµï¼ˆé·ç§»ã‚¿ã‚¤ãƒ—ã«å¿œã˜ã¦ï¼‰
    # é‡è¦: ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†å¾Œã«æ˜ åƒé·ç§»ã‚’é–‹å§‹ï¼ˆå­—å¹•ã¨ã®ã‚ºãƒ¬ã‚’é˜²ãï¼‰
    if transition_type == "ãªã—ï¼ˆã‚«ãƒƒãƒˆï¼‰" or len(clips) <= 1:
        print("   å…¨ã‚·ãƒ¼ãƒ³ã‚’é€£çµä¸­ï¼ˆã‚«ãƒƒãƒˆï¼‰...")
        final_clip = concatenate_videoclips(clips, method="compose")

    elif transition_type == "ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰":
        print(f"   å…¨ã‚·ãƒ¼ãƒ³ã‚’é€£çµä¸­ï¼ˆã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰: {transition_duration}ç§’ï¼‰...")
        # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†å¾Œã«ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰é–‹å§‹
        transition_clips = []
        for i, clip in enumerate(clips):
            if i == 0:
                # æœ€åˆã®ã‚¯ãƒªãƒƒãƒ—ã¯ãã®ã¾ã¾
                transition_clips.append(clip)
            else:
                # å‰ã®ã‚¯ãƒªãƒƒãƒ—ã®æœ€å¾Œã«ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆã‚’è¿½åŠ ï¼ˆéŸ³å£°ãªã—ï¼‰
                prev_clip = transition_clips[-1]

                # å‰ã®ã‚¯ãƒªãƒƒãƒ—ã®æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ä½¿ã£ã¦é·ç§»ç”¨ã®æ˜ åƒã®ã¿ã‚¯ãƒªãƒƒãƒ—ã‚’ä½œæˆ
                # éŸ³å£°ã¯å«ã¾ãªã„ï¼ˆwithout_audioï¼‰
                prev_last_frame = prev_clip.to_ImageClip(prev_clip.duration).with_duration(transition_duration)
                prev_fade = prev_last_frame.with_effects([vfx.FadeOut(transition_duration)])

                # ç¾åœ¨ã®ã‚¯ãƒªãƒƒãƒ—ã®æœ€åˆã«ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¤ãƒ³ã‚’è¿½åŠ 
                current_first_frame = clip.to_ImageClip(0).with_duration(transition_duration)
                current_fade = current_first_frame.with_effects([vfx.FadeIn(transition_duration)])

                # ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰éƒ¨åˆ†ã‚’åˆæˆï¼ˆéŸ³å£°ãªã—ï¼‰
                crossfade = CompositeVideoClip([prev_fade, current_fade], size=prev_fade.size)

                # å‰ã®ã‚¯ãƒªãƒƒãƒ— + ã‚¯ãƒ­ã‚¹ãƒ•ã‚§ãƒ¼ãƒ‰ + ç¾åœ¨ã®ã‚¯ãƒªãƒƒãƒ—ã‚’é€£çµ
                transition_clips.append(crossfade)
                transition_clips.append(clip)

        final_clip = concatenate_videoclips(transition_clips, method="compose")

    elif transition_type == "ã‚¹ãƒ©ã‚¤ãƒ‰":
        print(f"   å…¨ã‚·ãƒ¼ãƒ³ã‚’é€£çµä¸­ï¼ˆã‚¹ãƒ©ã‚¤ãƒ‰: {transition_duration}ç§’ï¼‰...")
        # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³çµ‚äº†å¾Œã«ã‚¹ãƒ©ã‚¤ãƒ‰é–‹å§‹
        transition_clips = []
        for i, clip in enumerate(clips):
            if i == 0:
                transition_clips.append(clip)
            else:
                # å‰ã®ã‚¯ãƒªãƒƒãƒ—ã®æœ€å¾Œã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§ãƒ•ã‚§ãƒ¼ãƒ‰ã‚¢ã‚¦ãƒˆï¼ˆéŸ³å£°ãªã—ï¼‰
                prev_clip = transition_clips[-1]
                prev_last_frame = prev_clip.to_ImageClip(prev_clip.duration).with_duration(transition_duration)
                prev_fade = prev_last_frame.with_effects([vfx.FadeOut(transition_duration)])

                # ç¾åœ¨ã®ã‚¯ãƒªãƒƒãƒ—ã®æœ€åˆã®ãƒ•ãƒ¬ãƒ¼ãƒ ã§ã‚¹ãƒ©ã‚¤ãƒ‰ã‚¤ãƒ³ï¼ˆéŸ³å£°ãªã—ï¼‰
                w, h = clip.size
                current_first_frame = clip.to_ImageClip(0).with_duration(transition_duration)
                current_slide = current_first_frame.with_position(lambda t: (int(w * (1 - t / transition_duration)), 0))

                # ã‚¹ãƒ©ã‚¤ãƒ‰éƒ¨åˆ†ã‚’åˆæˆï¼ˆéŸ³å£°ãªã—ï¼‰
                slide_transition = CompositeVideoClip([prev_fade, current_slide], size=(w, h))

                transition_clips.append(slide_transition)
                transition_clips.append(clip)

        final_clip = concatenate_videoclips(transition_clips, method="compose")

    else:
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚«ãƒƒãƒˆ
        print("   å…¨ã‚·ãƒ¼ãƒ³ã‚’é€£çµä¸­ï¼ˆã‚«ãƒƒãƒˆï¼‰...")
        final_clip = concatenate_videoclips(clips, method="compose")

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    output_file = output_dir / f"{book_name}_promotional_video.mp4"

    # å‹•ç”»ã‚’æ›¸ãå‡ºã—
    print(f"   å‹•ç”»ã‚’æ›¸ãå‡ºã—ä¸­: {output_file.name}")
    final_clip.write_videofile(
        str(output_file),
        fps=24,
        codec='libx264',
        audio_codec='aac',
        temp_audiofile='temp-audio.m4a',
        remove_temp=True,
        threads=4,
        preset='medium'
    )

    # ã‚¯ãƒªãƒƒãƒ—ã‚’è§£æ”¾
    final_clip.close()
    for clip in clips:
        clip.close()

    print(f"âœ… å‹•ç”»ç”Ÿæˆå®Œäº†ï¼ˆå­—å¹•ãªã—): {output_file}")

    # å­—å¹•ã‚’è¿½åŠ 
    if subtitle_type in ["normal", "karaoke"]:
        final_output_file = add_subtitles_to_video(
            output_file,
            scenes,
            subtitle_type,
            storyboard_data.get('aspect_ratio', '9:16'),
            subtitle_colors
        )
    else:
        final_output_file = output_file

    # å‹•ç”»æƒ…å ±ã‚’è¿”ã™
    video_data = {
        "book_name": book_name,
        "video_file": str(final_output_file),
        "subtitle_type": subtitle_type,
        "has_bgm": False,
        "total_scenes": len(scenes),
        "duration": sum([AudioFileClip(str(Path(s['audio_file']))).duration for s in scenes])
    }

    return video_data


def add_subtitles_to_video(
    video_file: Path,
    scenes: List[Dict[str, Any]],
    subtitle_type: str,
    aspect_ratio: str = "9:16",
    subtitle_colors: tuple = ("FFFFFF", "00FFFF")
) -> Path:
    """
    å‹•ç”»ã«ASSå­—å¹•ã‚’è¿½åŠ 

    Args:
        video_file: å…¥åŠ›å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«
        scenes: ã‚·ãƒ¼ãƒ³æƒ…å ±ã®ãƒªã‚¹ãƒˆ
        subtitle_type: å­—å¹•ã‚¿ã‚¤ãƒ— ("karaoke" or "normal")
        aspect_ratio: ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”

    Returns:
        å­—å¹•ä»˜ãå‹•ç”»ã®ãƒ‘ã‚¹
    """
    print(f"ğŸ“ å­—å¹•ã‚’è¿½åŠ ä¸­ï¼ˆ{subtitle_type}ï¼‰...")

    # ASSå­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
    subtitle_file = video_file.parent / f"{video_file.stem}_subtitles.ass"

    if subtitle_type == "karaoke":
        subtitle_generator.create_karaoke_subtitle_file(
            scenes,
            subtitle_file,
            max_chars=15,
            aspect_ratio=aspect_ratio,
            colors=subtitle_colors
        )
    else:
        subtitle_generator.create_normal_subtitle_file(
            scenes,
            subtitle_file,
            max_chars=20,
            aspect_ratio=aspect_ratio
        )

    print(f"   å­—å¹•ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆå®Œäº†: {subtitle_file.name}")

    # ffmpegã§å­—å¹•ã‚’ç„¼ãè¾¼ã¿
    output_file = video_file.parent / f"{video_file.stem}_with_subtitles.mp4"

    ffmpeg_cmd = [
        'ffmpeg',
        '-i', str(video_file),
        '-vf', f"ass={subtitle_file}",
        '-c:v', 'libx264',
        '-c:a', 'copy',
        '-y',
        str(output_file)
    ]

    print(f"   å­—å¹•ã‚’å‹•ç”»ã«ç„¼ãè¾¼ã¿ä¸­...")
    result = subprocess.run(
        ffmpeg_cmd,
        capture_output=True,
        text=True
    )

    if result.returncode != 0:
        print(f"âš ï¸ å­—å¹•è¿½åŠ ã‚¨ãƒ©ãƒ¼ï¼ˆå­—å¹•ãªã—ã§ç¶šè¡Œï¼‰: {result.stderr[:200]}")
        return video_file

    print(f"âœ… å­—å¹•è¿½åŠ å®Œäº†: {output_file.name}")

    return output_file


def add_bgm_to_video(
    video_file: Path,
    bgm_file: Path,
    volume: float = 0.15
) -> Path:
    """
    å‹•ç”»ã«BGMã‚’è¿½åŠ 

    Args:
        video_file: å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        bgm_file: BGMãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        volume: BGMéŸ³é‡ (0.0 - 1.0)

    Returns:
        BGMä»˜ãå‹•ç”»ã®ãƒ‘ã‚¹
    """
    if not video_file.exists():
        raise FileNotFoundError(f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_file}")
    if not bgm_file.exists():
        raise FileNotFoundError(f"BGMãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {bgm_file}")

    print(f"ğŸµ BGMã‚’è¿½åŠ ä¸­: {bgm_file.name}")

    # å‹•ç”»ã‚’èª­ã¿è¾¼ã¿
    video = VideoFileClip(str(video_file))

    # BGMã‚’èª­ã¿è¾¼ã¿ï¼ˆå‹•ç”»ã®é•·ã•ã«ãƒ«ãƒ¼ãƒ—ï¼‰
    bgm = AudioFileClip(str(bgm_file))

    # éŸ³é‡èª¿æ•´
    from moviepy import afx
    bgm = bgm.with_effects([afx.MultiplyVolume(volume)])

    # BGMã‚’ãƒ«ãƒ¼ãƒ—ï¼ˆå‹•ç”»ã®é•·ã•ã«åˆã‚ã›ã‚‹ï¼‰
    if bgm.duration < video.duration:
        # BGMãŒçŸ­ã„å ´åˆã¯ãƒ«ãƒ¼ãƒ—
        loop_count = int(video.duration / bgm.duration) + 1
        from moviepy import concatenate_audioclips
        bgm = concatenate_audioclips([bgm] * loop_count)

    # BGMã‚’å‹•ç”»ã®é•·ã•ã«åˆ‡ã‚Šè©°ã‚
    bgm = bgm.subclipped(0, video.duration)

    # ãƒŠãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨BGMã‚’ãƒŸãƒƒã‚¯ã‚¹
    if video.audio:
        mixed_audio = CompositeAudioClip([video.audio, bgm])
        video = video.with_audio(mixed_audio)
    else:
        video = video.with_audio(bgm)

    # å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«å
    output_file = video_file.parent / f"{video_file.stem}_with_bgm.mp4"

    # å‹•ç”»ã‚’æ›¸ãå‡ºã—
    print(f"   BGMä»˜ãå‹•ç”»ã‚’æ›¸ãå‡ºã—ä¸­: {output_file.name}")
    video.write_videofile(
        str(output_file),
        fps=24,
        codec='libx264',
        audio_codec='aac',
        temp_audiofile='temp-audio-bgm.m4a',
        remove_temp=True,
        threads=4,
        preset='medium'
    )

    # ã‚¯ãƒªãƒƒãƒ—ã‚’è§£æ”¾
    video.close()
    bgm.close()

    print(f"âœ… BGMè¿½åŠ å®Œäº†: {output_file}")

    return output_file
